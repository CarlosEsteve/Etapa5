{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar librerias y definición de la ruta  de trabajo (path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[38;5;208mVersión skforecast: 0.15.1\n",
      "\u001b[1m\u001b[38;5;208mVersión scikit-learn: 1.6.1\n",
      "\u001b[1m\u001b[38;5;208mVersión pandas: 2.2.3\n",
      "\u001b[1m\u001b[38;5;208mVersión numpy: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Tratamiento de datos\n",
    "# ==============================================================================\n",
    "import shap\n",
    "from skforecast.utils import load_forecaster\n",
    "from skforecast.utils import save_forecaster\n",
    "from skforecast.preprocessing import RollingFeatures\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.model_selection import grid_search_forecaster\n",
    "from skforecast.model_selection import TimeSeriesFold\n",
    "from skforecast.direct import ForecasterDirect\n",
    "from skforecast.recursive import ForecasterRecursive\n",
    "import skforecast\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn\n",
    "from skforecast.datasets import fetch_dataset\n",
    "\n",
    "# Gráficos\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['lines.linewidth'] = 1.5\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Modelado y Forecasting\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Configuración warnings\n",
    "# ==============================================================================\n",
    "warnings.filterwarnings('once')\n",
    "\n",
    "color = '\\033[1m\\033[38;5;208m'\n",
    "print(f\"{color}Versión skforecast: {skforecast.__version__}\")\n",
    "print(f\"{color}Versión scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"{color}Versión pandas: {pd.__version__}\")\n",
    "print(f\"{color}Versión numpy: {np.__version__}\")\n",
    "\n",
    "# Formato de los prints\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "\n",
    "def headr(text):\n",
    "    return ('\\n'+color.UNDERLINE + text + color.END+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingresa la ruta donde está el repositorio\n",
    "ruta = 'c:/repo_remoto/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Características Equipos\n",
    "\n",
    "equipos = pd.read_csv(ruta + 'etapa4/data/Caracteristicas_Equipos.csv')\n",
    "equipos_df = pd.DataFrame(equipos)\n",
    "### Historicos Ordenes\n",
    "\n",
    "ordenes = pd.read_csv(ruta + 'etapa4/data/Historicos_Ordenes.csv')\n",
    "ordenes_df = pd.DataFrame(ordenes)\n",
    "### Registros Condiciones\n",
    "\n",
    "condiciones = pd.read_csv(ruta + 'etapa4/data/Registros_Condiciones.csv')\n",
    "condiciones_df = pd.DataFrame(condiciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar equipos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "equipos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el número de filas duplicadas\n",
    "print(\"Número de filas duplicadas:\", equipos_df.duplicated().sum())\n",
    "\n",
    "# Mostrar todas las filas duplicadas considerando el índice como parte de los datos\n",
    "duplicados = equipos_df[equipos_df.duplicated(keep=False)]\n",
    "duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar ordenes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordenes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino ID_Orden, establezco Fecha como índice y le doy formato datetime\n",
    "ordenes_df = ordenes_df.drop(columns=['ID_Orden']).set_index('Fecha')\n",
    "ordenes_df.index = pd.to_datetime(ordenes_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el número de filas duplicadas considerando el índice como parte de los datos\n",
    "print(\"Número de filas duplicadas (incluyendo índice como columna):\", ordenes_df.reset_index().duplicated(keep=False).sum())\n",
    "\n",
    "# Mostrar todas las filas duplicadas considerando el índice como parte de los datos\n",
    "duplicados = ordenes_df.reset_index()[ordenes_df.reset_index().duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratar condiciones_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condiciones_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino ID_Registro, establezco Fecha como índice y le doy formato datetime\n",
    "condiciones_df = condiciones_df.drop(columns=['ID_Registro']).set_index('Fecha')\n",
    "condiciones_df.index = pd.to_datetime(condiciones_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el número de filas duplicadas considerando el índice como parte de los datos\n",
    "print(\"Número de filas duplicadas (incluyendo índice como columna):\", condiciones_df.reset_index().duplicated(keep=False).sum())\n",
    "\n",
    "# Mostrar todas las filas duplicadas considerando el índice como parte de los datos\n",
    "duplicados = condiciones_df.reset_index()[condiciones_df.reset_index().duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurarse de que los índices sean de tipo datetime\n",
    "ordenes_df.index = pd.to_datetime(ordenes_df.index)\n",
    "condiciones_df.index = pd.to_datetime(condiciones_df.index)\n",
    "\n",
    "# Mezclar los DataFrames basándose en el índice (fechas) y la columna 'ID_Equipo'\n",
    "merged_df = pd.merge(\n",
    "    ordenes_df.reset_index(),  # Reseteamos el índice temporal para incluirlo como columna\n",
    "    condiciones_df.reset_index(),\n",
    "    on=['Fecha', 'ID_Equipo'],  # Mezclamos por la fecha y el ID_Equipo\n",
    "    how='outer'\n",
    ")\n",
    "\n",
    "# Volver a establecer 'Fecha' como índice\n",
    "merged_df.set_index('Fecha', inplace=True)\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(merged_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relleno la columna Tipo_Mantenimiento vacías con Preventivo\n",
    "merged_df['Tipo_Mantenimiento'] = merged_df['Tipo_Mantenimiento'].fillna('Preventivo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario con las ubicaciones conocidas por ID_Equipo\n",
    "ubicaciones_por_equipo = ordenes_df[['ID_Equipo', 'Ubicacion']].dropna().drop_duplicates().set_index('ID_Equipo')['Ubicacion'].to_dict()\n",
    "\n",
    "# Rellenar las filas vacías de Ubicacion en merged_df usando el diccionario\n",
    "merged_df['Ubicacion'] = merged_df.apply(\n",
    "    lambda row: ubicaciones_por_equipo.get(row['ID_Equipo'], row['Ubicacion']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Verificar si quedan valores vacíos en la columna Ubicacion\n",
    "print(\"Valores vacíos restantes en 'Ubicacion':\", merged_df['Ubicacion'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino columnas Costo_Mantenimiento y Duración_Horas\n",
    "merged_df = merged_df.drop(columns=['Costo_Mantenimiento', 'Duracion_Horas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar merged_df con equipos_df usando ID_Equipo como clave\n",
    "merged_df = pd.merge(\n",
    "    merged_df.reset_index(),  # Reseteamos el índice temporal para incluirlo como columna\n",
    "    equipos_df,               # Información de los equipos\n",
    "    on='ID_Equipo',           # Clave de unión\n",
    "    how='left'                # Unión izquierda para mantener todas las filas de merged_df\n",
    ")\n",
    "\n",
    "# Volver a establecer 'Fecha' como índice\n",
    "merged_df.set_index('Fecha', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar el resultado\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Extraer el DataFrame unido a un archivo CSV\n",
    "\n",
    "# merged_df = pd.DataFrame(merged_df)\n",
    "\n",
    "# merged_df.to_csv(ruta + 'etapa4/output/merge_df.csv', index=True)\n",
    "\n",
    "\n",
    "# print(\"El DataFrame merged_df se ha extraído a *.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis y exportación Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv(ruta + 'Etapa4/output/merge_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(merged_df, title=\"Mantenimiento Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(ruta + 'Etapa4/output/Mantenimiento.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Este notebook realiza un análisis exhaustivo de datos relacionados con el mantenimiento de equipos, abarcando desde la carga y limpieza de datos hasta la generación de reportes de análisis. A continuación, se detalla cada sección:\n",
    "\n",
    "---\n",
    "\n",
    "1. **Carga y preparación de datos**:\n",
    "   - Se importan librerías necesarias y se definen rutas de trabajo.\n",
    "   - Se cargan tres conjuntos de datos: características de equipos, históricos de órdenes y registros de condiciones.\n",
    "   - Se procesan los datos eliminando columnas irrelevantes, convirtiendo índices a formato de fecha y combinando los DataFrames en uno único (`merged_df`).\n",
    "\n",
    "2. **Limpieza de datos**:\n",
    "   - Se identifican y eliminan duplicados.\n",
    "   - Se rellenan valores faltantes en columnas clave como `Tipo_Mantenimiento` y `Ubicacion`.\n",
    "   - Se eliminan columnas innecesarias y se verifican posibles inconsistencias.\n",
    "\n",
    "3. **Integración y análisis**:\n",
    "   - Los datos se integran en un único DataFrame (`merged_df`) que combina información de equipos, órdenes y condiciones.\n",
    "   - Se genera un reporte de análisis exploratorio utilizando `ydata_profiling` para identificar patrones, valores atípicos y distribuciones.\n",
    "\n",
    "4. **Exportación de resultados**:\n",
    "   - El DataFrame final se exporta a un archivo CSV para su uso posterior.\n",
    "   - Se genera un reporte HTML con el análisis exploratorio.\n",
    "---\n",
    "#### Conclusión Final\n",
    "\n",
    "El notebook proporciona un flujo completo para el análisis de datos de mantenimiento, desde la preparación e integración de datos hasta la generación de reportes detallados. Este proceso permite identificar patrones clave en los datos y preparar un conjunto limpio y estructurado para futuros análisis o modelado predictivo. La metodología aplicada asegura la calidad de los datos y facilita la toma de decisiones basada en información confiable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
